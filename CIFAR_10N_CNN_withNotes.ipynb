{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR-10N_CNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linzhe001/tutorial_notebooks/blob/main/CIFAR_10N_CNN_withNotes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cloning repository of CIFAR-10H Annotation\n",
        "Paper: Human uncertainty makes classification more robust (https://arxiv.org/pdf/1908.07086.pdf)\n",
        "\n",
        "Label:  CIFAR10 [0: airplane, 1: automobile, 2: bird, 3: cat, 4: deer, 5: dog, 6: frog, 7: horse, 8: ship, 9: truck] <br>\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1010/1*r8S5tF_6naagKOnlIcGXoQ.png\" alt=\"alternatetext\">\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_vL2RMWddkaQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KrQIqW6_NYl",
        "outputId": "76e21385-103a-46ba-8719-58e1f5936827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'cifar-10-100n' already exists and is not an empty directory.\n",
            "/content/cifar-10-100n\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/UCSC-REAL/cifar-10-100n.git\n",
        "%cd cifar-10-100n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main script"
      ],
      "metadata": {
        "id": "qNN4TybGd2Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import argparse\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def seed_everything(seed=12):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "## seed is used to ristrict randomness for the reproducibility purpose\n",
        "\n",
        "parser = argparse.ArgumentParser(description='CIFAR-10H Training') ## create an argparse object\n",
        "parser.add_argument('--lr', default=0.1, type=float, help='learning rate') ## add some augments\n",
        "parser.add_argument('--lr_schedule', default=0, type=int, help='lr scheduler')\n",
        "parser.add_argument('--batch_size', default=1024, type=int, help='batch size')\n",
        "parser.add_argument('--test_batch_size', default=2048, type=int, help='batch size')\n",
        "parser.add_argument('--num_epoch', default=100, type=int, help='epoch number')\n",
        "parser.add_argument('--num_classes', type=int, default=10, help='number classes')\n",
        "args = parser.parse_args(args=[]) ## store theses data in args.Argument; can print by ``` args.lr ```\n",
        "## notice ``` parser.parse_args ``` is necessary for this section, can not run ```parser.lr```\n",
        "\n",
        "def train(model, trainloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets, ad) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "NN371ewSCWX7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(args.lr)"
      ],
      "metadata": {
        "id": "ZWWLrLr1pcv2",
        "outputId": "70ef487c-3490-44ee-b219-b45f1c2a9816",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR-10H dataloader"
      ],
      "metadata": {
        "id": "WHKUA3yBd87G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torchvision\n",
        "\n",
        "class CIFAR10N(torchvision.datasets.CIFAR10):\n",
        "\n",
        "    def __init__(self, root,  rand_number=0, train=False, transform=None, target_transform=None,\n",
        "                 download=False, istrain=False):\n",
        "        super(CIFAR10N, self).__init__(root, train, transform, target_transform, download)\n",
        "        self.istrain = istrain\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        ann_all = torch.load('./data/CIFAR-10_human.pt')\n",
        "        ann_ = np.array([ann_all['random_label1'], ann_all['random_label2'], ann_all['random_label3']]).transpose(1,0)\n",
        "        self.ad = np.zeros((50000, 10))\n",
        "        for idx, ann_per_img in enumerate(ann_):\n",
        "            for ann_per_rater in ann_per_img:\n",
        "                self.ad[idx, ann_per_rater] += 1\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "        if self.istrain:\n",
        "            ad = self.ad[index]\n",
        "            return img, target, ad\n",
        "        else:\n",
        "            return img, target\n"
      ],
      "metadata": {
        "id": "z4uiWMiyCkjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run script"
      ],
      "metadata": {
        "id": "wRQEQIdaerKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything()\n",
        "mean_cifar10, std_cifar10 = (0.5071, 0.4866, 0.4409), (0.2009, 0.1984, 0.2023)\n",
        "transform_train = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n",
        "            transforms.Normalize(mean_cifar10, std_cifar10), ])\n",
        "transform_test = transforms.Compose([transforms.ToTensor(),\n",
        "    transforms.Normalize(mean_cifar10, std_cifar10),])\n",
        "\n",
        "train_dataset = CIFAR10N(root='./data', train=True, download=True, transform=transform_train, istrain=True)\n",
        "test_dataset = CIFAR10N(root='./data', train=False, download=True, transform=transform_test, istrain=False)\n",
        "#test_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_test)\n",
        "print('train samples:',len(train_dataset), 'test samples:',len(test_dataset))\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "model = models.resnet34(pretrained=True).to(device)\n",
        "model.fc = nn.Linear(model.fc.in_features, args.num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, nesterov=False, weight_decay=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "best_epoch, best_acc = 0.0, 0\n",
        "for epoch in range(args.num_epoch):\n",
        "    train(model, train_loader, criterion, optimizer)\n",
        "    accuracy = test(model, test_loader)\n",
        "    if accuracy > best_acc:\n",
        "        patience = 0\n",
        "        best_acc = accuracy\n",
        "        best_epoch = epoch\n",
        "        best_model = copy.deepcopy(model)\n",
        "        torch.save(best_model.state_dict(), 'best_model_cifar10h.pth.tar')\n",
        "    print('epoch: {}  acc: {:.4f}  best epoch: {}  best acc: {:.4f}'.format(\n",
        "            epoch, accuracy, best_epoch, best_acc, optimizer.param_groups[0]['lr']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rpOrSuEFY2N",
        "outputId": "cab3b1ed-0dcc-44f2-fd61-7da51d65f340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 12956801.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-8fdc7fd4e1ad>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ann_all = torch.load('./data/CIFAR-10_human.pt')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "train samples: 50000 test samples: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 145MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0  acc: 0.6230  best epoch: 0  best acc: 0.6230\n",
            "epoch: 1  acc: 0.7623  best epoch: 1  best acc: 0.7623\n",
            "epoch: 2  acc: 0.7848  best epoch: 2  best acc: 0.7848\n",
            "epoch: 3  acc: 0.8146  best epoch: 3  best acc: 0.8146\n",
            "epoch: 4  acc: 0.8140  best epoch: 3  best acc: 0.8146\n",
            "epoch: 5  acc: 0.7897  best epoch: 3  best acc: 0.8146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_probability as tfp\n",
        "\n",
        "def evaluation_all(model, testloader):\n",
        "    model.eval()\n",
        "    logits_list = []\n",
        "    labels_list = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            logits_list.append(outputs)\n",
        "            labels_list.append(targets)\n",
        "\n",
        "        logits = torch.cat(logits_list).cpu().numpy()\n",
        "        labels = torch.cat(labels_list).cpu().numpy()\n",
        "    return correct / total, logits, labels\n",
        "\n",
        "model.load_state_dict(torch.load('best_model_cifar10h.pth.tar'))\n",
        "acc, logits_tf, labels_tf = evaluation_all(model, test_loader)\n",
        "ece = tfp.stats.expected_calibration_error(args.num_classes, logits=logits_tf, labels_true=labels_tf, labels_predicted=np.argmax(logits_tf,1))\n",
        "print(\"Acc:{:.4f}, ECE:{:.4f}\".format(acc, np.array(ece)))"
      ],
      "metadata": {
        "id": "krBAqR41UU7p",
        "outputId": "18db5611-2211-4a83-f513-7789d3a059b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc:0.8076, ECE:0.0799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tz8KYjfqIzT8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}