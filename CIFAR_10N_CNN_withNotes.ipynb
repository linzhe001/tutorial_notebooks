{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linzhe001/tutorial_notebooks/blob/Notes/CIFAR_10N_CNN_withNotes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vL2RMWddkaQ"
      },
      "source": [
        "# Cloning repository of CIFAR-10H Annotation\n",
        "Paper: Human uncertainty makes classification more robust (https://arxiv.org/pdf/1908.07086.pdf)\n",
        "\n",
        "Label:  CIFAR10 [0: airplane, 1: automobile, 2: bird, 3: cat, 4: deer, 5: dog, 6: frog, 7: horse, 8: ship, 9: truck] <br>\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1010/1*r8S5tF_6naagKOnlIcGXoQ.png\" alt=\"alternatetext\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KrQIqW6_NYl",
        "outputId": "76e21385-103a-46ba-8719-58e1f5936827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'cifar-10-100n' already exists and is not an empty directory.\n",
            "/content/cifar-10-100n\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/UCSC-REAL/cifar-10-100n.git\n",
        "%cd cifar-10-100n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFQ3-QoLgzFL"
      },
      "source": [
        "![image](https://github.com/linzhe001/tutorial_notebooks/blob/Notes/media/CIFAR_10N_CNN%20code%20flow%20chart.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNN4TybGd2Q_"
      },
      "source": [
        "# main script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN371ewSCWX7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import argparse\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def seed_everything(seed=12):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "# seed is used to ristrict randomness for the reproducibility purpose\n",
        "\n",
        "parser = argparse.ArgumentParser(description='CIFAR-10H Training') #create an argparse object\n",
        "parser.add_argument('--lr', default=0.1, type=float, help='learning rate') #add some augments\n",
        "parser.add_argument('--lr_schedule', default=0, type=int, help='lr scheduler')\n",
        "parser.add_argument('--batch_size', default=1024, type=int, help='batch size')\n",
        "parser.add_argument('--test_batch_size', default=2048, type=int, help='batch size')\n",
        "parser.add_argument('--num_epoch', default=100, type=int, help='epoch number')\n",
        "parser.add_argument('--num_classes', type=int, default=10, help='number classes')\n",
        "args = parser.parse_args(args=[]) #store theses data in args.Argument; can print by ```args.lr```\n",
        "# notice ```parser.parse_args``` is necessary for this section, can not run ```parser.lr```\n",
        "\n",
        "def train(model, trainloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets, ad) in enumerate(trainloader):\n",
        "# enumerate will return two values: index and tuple; index is named batch idx, tuple is (inputs, targets, ad)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "# ```data.to(device)``` is the function in Pytoch, means data stay in CPU or move to GPU. depends on device\n",
        "        optimizer.zero_grad()\n",
        "# reset the gradients as 0, beacuse in Pytorch, gradients are accumulated by default during each iteration\n",
        "        outputs = model(inputs) ## forward pass: run this model using inputs to get output\n",
        "        loss = criterion(outputs, targets)\n",
        "# criterion is a loss function: used to calculate the difference outputs and targets\n",
        "        loss.backward()\n",
        "# calculate the gradient of the loss function\n",
        "## gradient here means: partial derivative of the loss function with a specific parameter; all parameters have their own gradient\n",
        "        optimizer.step()\n",
        "# optimizer use gradient``.grad`` calculated by ```loss.backwar()```\n",
        "# summary 1.forward pass to get output 2.compare output with target to get loss 3.calculate gradient by backward propagation 4.optimize the parameter\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval() #change model to evaluation module\n",
        "    correct = 0\n",
        "    total = 0 #intial correct test number and total test number for calculating accuarcy\n",
        "    with torch.no_grad(): #ban gradient calculation to boost computing\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader): #similar with train function\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs) #forward\n",
        "            _, predicted = outputs.max(1) #the output prediction, max possibility for classficiation\n",
        "            total += targets.size(0) #calculate the total test number\n",
        "# ```targets.size(0)``` is the length of this tensor, namely batch size eg.32 tests each batch.\n",
        "            correct += predicted.eq(targets).sum().item() #calculate the correct test number\n",
        "# ```predicted.eq(targets)``` compare predicted with targets and return True or False\n",
        "## ```.sum()``` used to sum Boolean value, True: 1; False: 0\n",
        "## ```.item()``` return tensor with only single value to integer\n",
        "    return correct / total #accuarcy\n",
        "#summary 1.forward pass to get output 2.compare output with targert 3.calculate accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHKUA3yBd87G"
      },
      "source": [
        "# CIFAR-10H_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4uiWMiyCkjk"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torchvision\n",
        "\n",
        "class CIFAR10N(torchvision.datasets.CIFAR10): #obtain properties from torchvision.datasets.CIFAR10\n",
        "\n",
        "    def __init__(self, root,  rand_number=0, train=False, transform=None, target_transform=None,\n",
        "                 download=False, istrain=False):\n",
        "        super(CIFAR10N, self).__init__(root, train, transform, target_transform, download)\n",
        "# call CIFAR10N's function ```__init__``` to initialize the CIFAR10N dataset\n",
        "        self.istrain = istrain\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform #assign value for ```__getitem__``` function from intial input\n",
        "        ann_all = torch.load('./data/CIFAR-10_human.pt') #.pt is pytorch file\n",
        "        ann_ = np.array([ann_all['random_label1'], ann_all['random_label2'], ann_all['random_label3']]).transpose(1,0)\n",
        "# transfer 3 label dictionary to a 3-dim numpy array and transpose it\n",
        "## random_label[n] means different people labeled\n",
        "        self.ad = np.zeros((50000, 10))\n",
        "        for idx, ann_per_img in enumerate(ann_):\n",
        "            for ann_per_rater in ann_per_img:\n",
        "# ann_per_rater contains the number of label eg.4, so give the loction to self.ad[] eg.self.ad[index,4]\n",
        "                self.ad[idx, ann_per_rater] += 1\n",
        "# summary 1.initialize the CIFAR10N dataset 2.load the labels from 3 rater 3.calculate the label number from human of each classfication\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "# both .data and .targets are from parent class, `.data` stores images eg.32x32x3 numpy array `.targets` stores labels\n",
        "        img = Image.fromarray(img) #change images from numpy arrary to PIL image\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img) #includes multiple transforms eg.resize\n",
        "# data augmentation and preprocessing\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target) #change label's format eg.one-hot encoding\n",
        "        if self.istrain:\n",
        "            ad = self.ad[index] #get human labels during training\n",
        "            return img, target, ad\n",
        "# compare targert and ad: target is hard label which is correct; ad is soft labels by human\n",
        "## target is traditional one; ad used to considering label uncertainty like noise\n",
        "        else:\n",
        "            return img, target\n",
        "# summary 1.get image and target labels into different container 2.transform image and target labels 3.if in train involve human labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRQEQIdaerKa"
      },
      "source": [
        "# Run script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rpOrSuEFY2N",
        "outputId": "cab3b1ed-0dcc-44f2-fd61-7da51d65f340"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 12956801.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-8fdc7fd4e1ad>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ann_all = torch.load('./data/CIFAR-10_human.pt')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "train samples: 50000 test samples: 10000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 145MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0  acc: 0.6230  best epoch: 0  best acc: 0.6230\n",
            "epoch: 1  acc: 0.7623  best epoch: 1  best acc: 0.7623\n",
            "epoch: 2  acc: 0.7848  best epoch: 2  best acc: 0.7848\n",
            "epoch: 3  acc: 0.8146  best epoch: 3  best acc: 0.8146\n",
            "epoch: 4  acc: 0.8140  best epoch: 3  best acc: 0.8146\n",
            "epoch: 5  acc: 0.7897  best epoch: 3  best acc: 0.8146\n"
          ]
        }
      ],
      "source": [
        "seed_everything()\n",
        "mean_cifar10, std_cifar10 = (0.5071, 0.4866, 0.4409), (0.2009, 0.1984, 0.2023)\n",
        "transform_train = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n",
        "            transforms.Normalize(mean_cifar10, std_cifar10), ]) #the function from torchvision.transforms\n",
        "# Augmentation and preprocess\n",
        "transform_test = transforms.Compose([transforms.ToTensor(),\n",
        "    transforms.Normalize(mean_cifar10, std_cifar10),])\n",
        "# only preprocess\n",
        "\n",
        "train_dataset = CIFAR10N(root='./data', train=True, download=True, transform=transform_train, istrain=True)\n",
        "test_dataset = CIFAR10N(root='./data', train=False, download=True, transform=transform_test, istrain=False)\n",
        "# use the dataloader 'CIFAR10N' to get datasets\n",
        "\n",
        "#test_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_test)\n",
        "print('train samples:',len(train_dataset), 'test samples:',len(test_dataset))\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False, num_workers=2)\n",
        "# dataloader get data from datasets on demond: batch_size, shuffle or not, multiple processes in parallel\n",
        "\n",
        "model = models.resnet34(pretrained=True).to(device) #get the model\n",
        "model.fc = nn.Linear(model.fc.in_features, args.num_classes) #change the fc layer to make sure the number output features is same with number of classes\n",
        "model = model.to(device) #move model to GPU or CPU\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, nesterov=False, weight_decay=0.0001) #optimizer\n",
        "criterion = nn.CrossEntropyLoss() #loss function used in train function\n",
        "\n",
        "best_epoch, best_acc = 0.0, 0\n",
        "for epoch in range(args.num_epoch):\n",
        "    train(model, train_loader, criterion, optimizer) # iterations = number of sample in trainsets / batch_size in each epoch\n",
        "    accuracy = test(model, test_loader)\n",
        "    if accuracy > best_acc:\n",
        "        patience = 0\n",
        "        best_acc = accuracy\n",
        "        best_epoch = epoch\n",
        "        best_model = copy.deepcopy(model)\n",
        "        torch.save(best_model.state_dict(), 'best_model_cifar10h.pth.tar')\n",
        "    print('epoch: {}  acc: {:.4f}  best epoch: {}  best acc: {:.4f}'.format(\n",
        "            epoch, accuracy, best_epoch, best_acc, optimizer.param_groups[0]['lr']))\n",
        "# train loop to find the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krBAqR41UU7p",
        "outputId": "18db5611-2211-4a83-f513-7789d3a059b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acc:0.8076, ECE:0.0799\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_probability as tfp\n",
        "\n",
        "def evaluation_all(model, testloader):\n",
        "    model.eval()\n",
        "    logits_list = []\n",
        "    labels_list = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            logits_list.append(outputs)\n",
        "            labels_list.append(targets)\n",
        "\n",
        "        logits = torch.cat(logits_list).cpu().numpy()\n",
        "        labels = torch.cat(labels_list).cpu().numpy()\n",
        "    return correct / total, logits, labels\n",
        "\n",
        "model.load_state_dict(torch.load('best_model_cifar10h.pth.tar'))\n",
        "acc, logits_tf, labels_tf = evaluation_all(model, test_loader)\n",
        "ece = tfp.stats.expected_calibration_error(args.num_classes, logits=logits_tf, labels_true=labels_tf, labels_predicted=np.argmax(logits_tf,1))\n",
        "print(\"Acc:{:.4f}, ECE:{:.4f}\".format(acc, np.array(ece)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz8KYjfqIzT8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "CIFAR-10N_CNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}